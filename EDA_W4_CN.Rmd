---
title: "Week 4 Classes"
author: "yob"
date: "5/6/2019"
output: pdf_document
---

##Week 4 Classes

## Clustering Case Study
Samsung data set

if you want to analyze by a factor, then convert the specific column into factor- class.

you can create sub-datasets by alienating each subject


- legend("bottomright", legend = unique(sub1$activity), col = unique(sub1$activity))

1)we can try to cluster the data for the first subject

distanceMatrix <- dist(sub1[, 1:3])

hclustering <- hclust(distanceMatrix)

myplclust(hclustering, lab.col = unclass(sub1$activity))


2)you can look at the plot of max acceleration for the first subject

cluster the data based on maximum acceleration. this way you get a definitive two cluster picture

it seems to separate moving from non moving

3)try singular value decomposition to see what's going on

clear non-value columns like subject id and activity type

first singular vector of U matrix seems an informative

4) find maximum contributor.
second right singular vector(V matrix)

which of the 500 features contributes the most variance

maxContrib <- which.max(svd1$v[, 2])
distanceMatrix <- dist(sub1[, c(10:12, maxContrib)])
hculstering....

5) new clustering with maximum contributer

names(samsungData)[maxContrib]

6) k-means clustering(nstart = 1, first try)
nstart should be more than 1. nstart = 100

kClust <- kmeans(sub1[, -c(562, 563)], centers = 6)

table(kClust$cluster, sub1$activity)

7) cluster1 variable centers (laying)

plot(kClust$center[1, 1:10], pch = 19, ylab = "Cluster Center", xlab = "")

you can look at the locations of the cluster centers. what are the centroid values

##Air Pollution Case Study

#asking the question

what are you looking for?

-are airpollution rates are lower than before?

http://goo.gl/soQZHM

#opening up and exploring data files

```{r}
pm0 <- read.table("RD_501_88101_1999-0.txt", comment.char = "#", header = FALSE, sep = "|", na.strings = "")
cnames<- readLines("RD_501_88101_1999-0.txt", 1)
#only RD variables present
print(cnames)
```


```{r}
cnames <- strsplit(cnames, "|", fixed = TRUE)
#it's a fixed pattern
names(pm0) <- make.names(cnames[[1]]) #to make the names valid. because normally they have spaces
#strsplit returns a list, so you want the first element of the list
head(pm0)
```

there are no RC records here. all RD records

easy to check from terminal
grep ^RC RD_501_88101_1999-0.txt

you can launch R at the terminal

```{r}
#let's look at sample value column
x0 <- pm0$Sample.Value
summary(x0)
```

```{r}
mean(is.na(x0))
#about %11 is missing.

#is missing values something I need to worry about?
```

missing values can play a very different role depending on what kind of question you're trying to answer.

#let's read the 2012 data

```{r}
pm1 <- read.table("RD_501_88101_2012-0.txt", comment.char = "#", header = F, sep = "|", na.strings = "")
```

much bigger monitoring network

```{r}
dim(pm1)
```
column names are going to be the same

```{r}
names(pm1) <- make.names(cnames[[1]])
head(pm1)
```

```{r}
x1 <- pm1$Sample.Value
str(x1)
```

```{r}
summary(x1)
```

```{r}
summary(x0)
```

quick look at summary difference

```{r}
mean(is.na(x1))
#fewer missing values. %5 missing is not a big deal
```

```{r}
boxplot(x0, x1)
```

900 as a value can be an error.

we're not focusing on extremes right now

```{r}
boxplot(log10(x0), log10(x1))
```

1999 data, median is at 1 which means 10 (mg). 

there are negative values. why is that? you can't have a negative mass.

```{r}
summary(x1)
```


```{r}
negative <- x1 < 0
str(negative)
```

```{r}
mean(negative, na.rm = TRUE)
```
about %2 of the data is negative/faulty

```{r}
dates <- pm1$Date
str(dates)
```
convert these into dates.

```{r}
dates <- as.Date(as.character(dates), "%Y%m%d")
str(dates)
```

```{r}
hist(dates, "month")
```
most of the measurements happen in winter and spring

```{r}
hist(dates[negative], "month")
```

where negative values occur.

it's not entirely clear what this tells us. 

maybe it's just measurement error. but it's %2 of the data. so don't worry now, maybe later.

# why don't we look at a specific location rather than looking at the whole country.

let's pick a state, and see.

new york

```{r}
site0 <- unique(subset(pm0, State.Code == 36, c(County.Code, Site.ID)))
site1 <- unique(subset(pm1, State.Code == 36, c(County.Code, Site.ID)))
head(site0)
```

paste county code and site ID
what are the specific monitor locations, existing on both data tables
```{r}
site0 <- paste(site0[,1], site0[,2], sep = ".")
site1 <- paste(site1[,1], site1[,2], sep = ".")
str(site0)
```

2012 vector only has 18 elements. 1999 had 33 combinations
what is the intersection between them
```{r}
both <- intersect(site0, site1)
both
```

10 county/monitors are existent between 1999 - 2012

let's look at how many observations are in each county monitors

```{r}
pm0$county.site <- with(pm0, paste(County.Code, Site.ID, sep = "."))
pm1$county.site <- with(pm1, paste(County.Code, Site.ID, sep = "."))
```
add the county monitors to original dataset

```{r}
#subset pm0 to be state code 36
cnt0 <- subset(pm0, State.Code == 36 & county.site %in% both)
cnt1 <- subset(pm1, State.Code == 36 & county.site %in% both)
```

```{r}
head(cnt0)
```

data of only New York

```{r}
#split the data frame by the monitors.
sapply(split(cnt0, cnt0$county.site), nrow)
```

number of observations in each county/monitor

```{r}
sapply(split(cnt1, cnt1$county.site), nrow)
```

the county/monitor to pick will be 63.2008

```{r}
pm1sub <- subset(pm1, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
pm0sub <- subset(pm0, State.Code == 36 & County.Code == 63 & Site.ID == 2008)
```

#take each of these data frames, and plot pm data with time.

#and visualise wether the levels of pm2.5 has gone down in this period

```{r}
dates1 <- pm1sub$Date
x1sub <- pm1sub$Sample.Value
plot(dates1, x1sub)
```
dates are not coded properly
```{r}
dates1 <- as.Date(as.character(dates1), "%Y%m%d")
str(dates1)
```

```{r}
plot(dates1, x1sub)
```

```{r}
dates0 <- pm0sub$Date
dates0 <- as.Date(as.character(dates0), "%Y%m%d")
x0sub <- pm0sub$Sample.Value
plot(dates0, x0sub)
```

hard to look separately

```{r}
par(mfrow = c(1, 2))#1 row 2 columns
plot(dates0, x0sub)
abline(h = median(x0sub, na.rm = T))
plot(dates1, x1sub)
abline(h = median(x1sub, na.rm = T))
```

the picture is misleading because the medians are not at the same place - olcek farkli


```{r}

rng <- range(x0sub, x1sub, na.rm = T)
range(x0sub, x1sub, na.rm = T)
```
```{r}
par(mfrow = c(1, 2))#1 row 2 columns
plot(dates0, x0sub, ylim = rng)
abline(h = median(x0sub, na.rm = T))
plot(dates1, x1sub, ylim = rng)
abline(h = median(x1sub, na.rm = T))
```

median is going down between the two years.

there is a huge spread in 1999 data, but a modest spread in 2012 data.

not only the average is going down, but also the extremes are going down.

one is a chronic problem, the other is an acute problem of spikes that can cause health problems

#exploring change at state level.

let's not look at the whole country, but it's not useful to look at one monitor. let's see the individual states. 

the states are where the regulations of EPA occurs. it's up to the state to figure out how to abide by the regulation changes.

let's create a plot with state averages of each year, and connect each state, whether if it's going up, down, or staying the same

```{r}
head(pm0)
```

take average value by State.Code from Sample.Value

```{r}
mn0 <- with(pm0, tapply(Sample.Value, State.Code, mean, na.rm = T))
str(mn0)
```

```{r}
summary(mn0)
```

```{r}
mn1 <- with(pm1, tapply(Sample.Value, State.Code, mean, na.rm = T))
str(mn1)
```

```{r}
summary(mn1)
```

data frame with each of these guys.

```{r}
d0 <- data.frame(state = names(mn0), mean = mn0)
d1 <- data.frame(state = names(mn1), mean = mn1)
head(d0)
```
```{r}
head(d1)
```

merge it with state names

```{r}
mrg <- merge(d0, d1, by = "state")
dim(mrg)
```

```{r}
head(mrg)
```

```{r}
par(mfrow = c(1, 1))
with(mrg, plot(rep(1999, 52), mrg[, 2], xlim= c(1998, 2013)))
with(mrg, points(rep(2012, 52), mrg[, 3]))
segments(rep(1999, 52), mrg[,2], rep(2012, 52), mrg[,3])
```

most of the states have gone down

we can see how each state has progressed in the years.

you can also see how the trends are in each state.

#the question
was has the pm2.5 value decreased through the years..

##Swirl


