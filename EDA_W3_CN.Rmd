---
title: "Week 3 Classes"
author: "yob"
date: "5/2/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

##Lesson 1
##Hierarchical Clustering

clustering organizes things that are close into groups

-how do we define close
-how do we group things
-how do we visualize the grouping
-how do we interpret the grouping

google "Cluster Analysis" for more info

hierarchical clustering:
an agglomerative approach
-find closest two things
-put them together
0find next closest

requires:
-a defined distance
-a merging approach

produces:
-a tree showing how close things are to each other


distance or similarity:
-continuous - euclidean distance
-continuous - correlation similarity
-binary - manhattan distance

pick a distance/similarity that makes sense for your problem

--distance examples slides--
euclidean distance is easy to generalize. extends very naturally

manhattan distance - grid approach 

```{r}
set.seed(1234)

par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x , y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

```

clustering algorithm
dist()
```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame) #default is euclidean
```

```{r}
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
```

you have to cut the tree at a designated height to get cluster number. at level of 1.0 there are 3 clusters. 
depending on where you draw the line you get more or fewer clusters.


#Part 3
```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

RGraphGallery for even prettier dendograms

# when you merge a point together, what is the new location? 
is the center of gravity in the middle?

complete linkage
average linkage

# heatmap()

```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)

```

if you have an extremely large table, and you want to take a quick look in an organised way...

this method should be primarily used for exploration

the clustering picture may be unstable. there may be outliers. you try different distances, merging strategies, scale of points for one variable maybe, change a few points...

choosing where to cut isn't always obvious. 

## K-means clustering

-how do we define close?
-how do we group things?
-how do we visualize the grouping
-how do we interpret what we see

k- means clustering

a partitioning approach

-fix a number of clusters
-get 'centriods' of each cluster 
-assign things to closest centroid
-recalculate centroids

requires 
-a defined distance metric
-a number of clusters
-an initial guess as to cluster centroids

produces
-funal estimate of cluster centroids
-an assigment of each point to a cluster

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x , y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```


centroids get closer and closer to real cluster areas in each computation.

#part 2 - kmeans()

important parameters: x, centers, iter.max, nstart

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)

```

```{r}
kmeansObj$cluster
```

```{r}
kmeansObj$centers
```


```{r}
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3,cex = 3, lwd = 3)

```

k-means clustering results above


--heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```

heatmap above

left is the original data.

right is the re-order of the rows of the data so that clusters are put together.

this way you can look at clusters in high dimensional data



#summary

k-means requires a number of clusters
-pick by eye/intuition
-pick by cross validation/information theory, etc.
-determining the number of clusters

k-means is not deterministic
-different # of clusters
-different number of iterations

-rafael irizarry's distances and clustering video
-elements of statistical learning

##Dimension Reduction - Principal Components Analysis and Singular Value Decomposition

```{r}
#matrix data

set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

no real interesting pattern, not surprisingly.


let's add a pattern

```{r}
set.seed(678910)
for (i in 1:40) {
        #flip a coin
        coinFlip <- rbinom(1, size = 1, prob = 0.5)
        #if coin is heads add a common pattern to that row
        if (coinFlip) {
                dataMatrix[i, ] <- dataMatrix[i , ] + rep(c(0, 3), each = 5)
        }
}
```

```{r}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[ , nrow(dataMatrix):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```


looking closer to patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab = "Row Mean",
     ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", 
     ylab = "Column Mean", pch = 19)
```


#related problems

-find a new set of multivariate variables that are uncorrelated and explain as much variance as possible

-if you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

the first goal is statistical, and the second goal is data compression


solutions : PCA/SVD

#the components of the SVD - u and v

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```

components of the SVD- variance explained

```{r}
par(mfrow = c(1,2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch  = 19)
```

the %40 of the variation is explained in the first dimension. remaining variation in the data is explained by other components

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1", ylab = "Right Singular Vector 1")
abline(c(0, 1))
```

svd and pca is basically the same things

#let's add a second pattern

```{r}
set.seed(678910)

for (i in 1:40) {
        #flip a coin
        coinFlip1 <- rbinom(1, size  = 1, prob = 0.5)
        coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
        #if coin is heads add a common pattern to that row
        if(coinFlip1) {
                dataMatrix[i, ]<- dataMatrix[i, ] + rep(c(0, 5), each = 5)
        }
        if(coinFlip2) {
                dataMatrix[i, ]<- dataMatrix[i, ] + rep(c(0, 5), 5)
        }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]

```


```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")
```

this is the truth we know of. we rarely know the truth 

```{r}
#the reality now
#v and patterns of variance in rows
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")
```
it's not as obvious as plotting the truth, but it captures it.

the two patterns are roughly compounded in each other, so it's a little harder to see.

the truth is always a little bit harder to discern

```{r}
#d and variance explained

svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained", pch = 19)
```

the first component explains over %50 of variance. because the shift pattern is so strong...

the second component only captures about 18% of the variation.


