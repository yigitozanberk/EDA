---
title: "Week 3 Classes"
author: "yob"
date: "5/2/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

##Lesson 1
##Hierarchical Clustering

clustering organizes things that are close into groups

-how do we define close
-how do we group things
-how do we visualize the grouping
-how do we interpret the grouping

google "Cluster Analysis" for more info

hierarchical clustering:
an agglomerative approach
-find closest two things
-put them together
0find next closest

requires:
-a defined distance
-a merging approach

produces:
-a tree showing how close things are to each other


distance or similarity:
-continuous - euclidean distance
-continuous - correlation similarity
-binary - manhattan distance

pick a distance/similarity that makes sense for your problem

--distance examples slides--
euclidean distance is easy to generalize. extends very naturally

manhattan distance - grid approach 

```{r}
set.seed(1234)

par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x , y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))

```

clustering algorithm
dist()
```{r}
dataFrame <- data.frame(x = x, y = y)
dist(dataFrame) #default is euclidean
```

```{r}
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
```

you have to cut the tree at a designated height to get cluster number. at level of 1.0 there are 3 clusters. 
depending on where you draw the line you get more or fewer clusters.


#Part 3
```{r}
dataFrame <- data.frame(x = x, y = y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab = rep(1:3, each = 4), lab.col = rep(1:3, each = 4))
```

RGraphGallery for even prettier dendograms

# when you merge a point together, what is the new location? 
is the center of gravity in the middle?

complete linkage
average linkage

# heatmap()

```{r}
dataFrame <- data.frame(x = x, y = y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)

```

if you have an extremely large table, and you want to take a quick look in an organised way...

this method should be primarily used for exploration

the clustering picture may be unstable. there may be outliers. you try different distances, merging strategies, scale of points for one variable maybe, change a few points...

choosing where to cut isn't always obvious. 

## K-means clustering

-how do we define close?
-how do we group things?
-how do we visualize the grouping
-how do we interpret what we see

k- means clustering

a partitioning approach

-fix a number of clusters
-get 'centriods' of each cluster 
-assign things to closest centroid
-recalculate centroids

requires 
-a defined distance metric
-a number of clusters
-an initial guess as to cluster centroids

produces
-funal estimate of cluster centroids
-an assigment of each point to a cluster

```{r}
set.seed(1234)
par(mar = c(0, 0, 0, 0))
x <- rnorm(12, mean = rep(1:3, each = 4), sd = 0.2)
y <- rnorm(12, mean = rep(c(1, 2, 1), each = 4), sd = 0.2)
plot(x , y, col = "blue", pch = 19, cex = 2)
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```


centroids get closer and closer to real cluster areas in each computation.

#part 2 - kmeans()

important parameters: x, centers, iter.max, nstart

```{r}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
names(kmeansObj)

```

```{r}
kmeansObj$cluster
```

```{r}
kmeansObj$centers
```


```{r}
par(mar = rep(0.2, 4))
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
points(kmeansObj$centers, col = 1:3, pch = 3,cex = 3, lwd = 3)

```

k-means clustering results above


--heatmaps

```{r}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix, centers = 3)
par(mfrow = c(1,2), mar = c(2, 4, 0.1, 0.1))
image(t(dataMatrix)[, nrow(dataMatrix):1], yaxt = "n")
image(t(dataMatrix)[, order(kmeansObj$cluster)], yaxt = "n")
```

heatmap above

left is the original data.

right is the re-order of the rows of the data so that clusters are put together.

this way you can look at clusters in high dimensional data



#summary

k-means requires a number of clusters
-pick by eye/intuition
-pick by cross validation/information theory, etc.
-determining the number of clusters

k-means is not deterministic
-different # of clusters
-different number of iterations

-rafael irizarry's distances and clustering video
-elements of statistical learning

##Dimension Reduction - Principal Components Analysis and Singular Value Decomposition

```{r}
#matrix data

set.seed(12345)
par(mar = rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow = 40)
image(1:10, 1:40, t(dataMatrix)[, nrow(dataMatrix):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```

no real interesting pattern, not surprisingly.


let's add a pattern

```{r}
set.seed(678910)
for (i in 1:40) {
        #flip a coin
        coinFlip <- rbinom(1, size = 1, prob = 0.5)
        #if coin is heads add a common pattern to that row
        if (coinFlip) {
                dataMatrix[i, ] <- dataMatrix[i , ] + rep(c(0, 3), each = 5)
        }
}
```

```{r}
par(mar = rep(0.2, 4))
image(1:10, 1:40, t(dataMatrix)[ , nrow(dataMatrix):1])
```

```{r}
par(mar = rep(0.2, 4))
heatmap(dataMatrix)
```


looking closer to patterns in rows and columns

```{r}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1, xlab = "Row Mean",
     ylab = "Row", pch = 19)
plot(colMeans(dataMatrixOrdered), xlab = "Column", 
     ylab = "Column Mean", pch = 19)
```


#related problems

-find a new set of multivariate variables that are uncorrelated and explain as much variance as possible

-if you put all the variables together in one matrix, find the best matrix created with fewer variables (lower rank) that explains the original data

the first goal is statistical, and the second goal is data compression


solutions : PCA/SVD

#the components of the SVD - u and v

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1,3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
```

components of the SVD- variance explained

```{r}
par(mfrow = c(1,2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch  = 19)
```

the %40 of the variation is explained in the first dimension. remaining variation in the data is explained by other components

```{r}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale = TRUE)
plot(pca1$rotation[, 1], svd1$v[, 1], pch = 19, xlab = "Principal Component 1", ylab = "Right Singular Vector 1")
abline(c(0, 1))
```

svd and pca is basically the same things

#let's add a second pattern

```{r}
set.seed(678910)

for (i in 1:40) {
        #flip a coin
        coinFlip1 <- rbinom(1, size  = 1, prob = 0.5)
        coinFlip2 <- rbinom(1, size = 1, prob = 0.5)
        #if coin is heads add a common pattern to that row
        if(coinFlip1) {
                dataMatrix[i, ]<- dataMatrix[i, ] + rep(c(0, 5), each = 5)
        }
        if(coinFlip2) {
                dataMatrix[i, ]<- dataMatrix[i, ] + rep(c(0, 5), 5)
        }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order, ]

```


```{r}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each = 5), pch = 19, xlab = "Column", ylab = "Pattern 1")
plot(rep(c(0, 1), 5), pch = 19, xlab = "Column", ylab = "Pattern 2")
```

this is the truth we know of. we rarely know the truth 

```{r}
#the reality now
#v and patterns of variance in rows
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 3))
image(t(dataMatrixOrdered)[, nrow(dataMatrixOrdered):1])
plot(svd2$v[, 1], pch = 19, xlab = "Column", ylab = "First right singular vector")
plot(svd2$v[, 2], pch = 19, xlab = "Column", ylab = "Second right singular vector")
```
it's not as obvious as plotting the truth, but it captures it.

the two patterns are roughly compounded in each other, so it's a little harder to see.

the truth is always a little bit harder to discern

```{r}
#d and variance explained

svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow = c(1, 2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Percent of variance explained", pch = 19)
```

the first component explains over %50 of variance. because the shift pattern is so strong...

the second component only captures about 18% of the variation.

#Dimension reduction part 3

one issue with svd or pca is always missing values

```{r}
dataMatrix2 <- dataMatrixOrdered
#randomly insert some missing data

dataMatrix2[sample(1:100, size = 40, replace = FALSE)] <- NA
svd1 <- svd(scale(dataMatrix2)) #Doesn't work!
```

#one solution - imputing

```{r}
BiocManager::install("impute")
library(impute)
```

```{r}
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered)); svd2 <- svd(scale(dataMatrix2))
par(mfrow= c(1,2)); plot(svd1$v[,1], pch = 19); plot(svd2$v[,1], pch = 19)

```


takes a missing row and imputes by k nearest neighbours. if k is 5, then it will take 5 rows closest, and impute the data with average of the values.

data on the left is from the original matrix. data on the right is from the imputed data

--face example

if you use 5-10 singular vectors, you get the approximation of the face image

data compression and statistical summaries are two sides of the same coin

#notes and further resources on SVD and PCA
-scale matters

-PCs/SVs may mix real patterns

-can be computationally intensive(if you have a very large matrix)

-alternatives:
factor analysis
independent components analysis
latent semantic analysis

##Lesson 3
##Working With Color in R plots

grDevices package has two functions
colorRamp
colorRampPalette

these functions take palettes of colors and help interpolate between the colors

the function colors() lists the names of colors you can use in any plotting function.

```{r}
pal <- colorRamp(c("red", "blue"))
pal(0)
```

```{r}
pal(seq(0, 1, len = 10))
```

colorRampPalette is similar with slight differences

```{r}
pal <- colorRampPalette(c("red", "yellow"))

pal(2)
```

```{r}
pal(10)
```

first two digits : red
second two digits : green
third two digits : blue

character vector of length 10

#RColorBrewer Package - very useful!!!!!!!!!

one package on CRAN that contains interesting/useful color palettes

there are 3 types of palettes
-sequential : data from low to high
-diverging : data that diverge, or deviate from a value(from a mean maybe - can get bigger in two different directions)
-qualitative : just used to represent data that are not ordered. like factors.

palette information can be used in conjunction with the colorRamp()  and colorRampPalette()

RColorBrewer color table
top group is for sequential
middle group is for qualitative 
bottom group is for diverging

```{r}
library(RColorBrewer)
cols <- brewer.pal(3, "BuGn")
cols
```

```{r}
pal <- colorRampPalette(cols)
image(volcano, col = pal(20))
```


#smoothScatter function

```{r}
x <- rnorm(10000)
y <- rnorm(10000)
smoothScatter(x, y)
```

2-D histogram of points. default palette is blues. darker blue for higher density, lighter blue for lower density.

rgb function can be used to produce any color via red, green, blue proportions

color transparency can be added via the alpha parameter to rgb. value between 0 and 1. 1 is not transparent. 0 is fully transparent

```{r}
plot(x, y, pch = 19)
```

```{r}
plot(x, y, col = rgb(0, 0, 0, 0.2), pch = 19)
#red= 0, green= 0, blue= 0, and alpha = 0.2
```

##Swirl Excercises
hierarchical clustering
k means clustering
dimension reduction
clustering example

#Hierarchical Clustering

```{r}
install.packages("fields")
library(fields)
```

| In this lesson we'll learn about hierarchical clustering, a simple way of
| quickly examining and displaying multi-dimensional data. This technique is
| usually most useful in the early stages of analysis when you're trying to get
| an understanding of the data, e.g., finding some pattern or relationship
| between different factors or variables. As the name suggests hierarchical
| clustering creates a hierarchy of clusters.


| We can keep going like this in the obvious way and pair up individual points,
| but as luck would have it, R provides a simple function which you can call
| which creates a dendrogram for you. It's called hclust() and takes as an
| argument the pairwise distance matrix which we looked at before. We've stored
| this matrix for you in a variable called distxy. Run hclust now with distxy as
| its argument and put the result in the variable hc.


